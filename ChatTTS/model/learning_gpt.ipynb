{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方便查看源代码\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Model,\n",
    ")\n",
    "GPT2Model.forward\n",
    "GPT2LMHeadModel.forward\n",
    "EncoderDecoderModel.forward\n",
    "EncoderDecoderModel.generate\n",
    "GPT2LMHeadModel.prepare_inputs_for_generation\n",
    "        \n",
    "import resource\n",
    "import time\n",
    "\n",
    "def print_resource_usage():\n",
    "    # 获取最大内存使用量（以字节为单位）\n",
    "    max_memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    # 打印内存使用情况\n",
    "    print(f\"Max Memory Usage: {max_memory_usage / 1024} MB\")\n",
    "\n",
    "    # 获取用户模式和系统模式下的CPU时间\n",
    "    user_time, sys_time = resource.getrusage(resource.RUSAGE_SELF)[:2]\n",
    "    # 打印CPU时间\n",
    "    print(f\"User Mode CPU Time: {user_time} seconds\")\n",
    "    print(f\"System Mode CPU Time: {sys_time} seconds\")\n",
    "\n",
    "        # 如果CUDA设备可用，打印CUDA内存使用情况\n",
    "    if torch.cuda.is_available():\n",
    "        # 获取CUDA设备的总内存和空闲内存\n",
    "        free_memory, total_memory = torch.cuda.mem_get_info()\n",
    "        # 计算已使用的内存\n",
    "        used_memory = total_memory - free_memory\n",
    "        # 打印CUDA内存使用情况\n",
    "        print(f\"CUDA Memory Usage: {used_memory / 1024**2} MB used out of {total_memory / 1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.logits_process import LogitsProcessorList, TemperatureLogitsWarper, TopKLogitsWarper, TopPLogitsWarper, RepetitionPenaltyLogitsProcessor\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# 挑选最适合的设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 示例使用\n",
    "model_name = \"openai-community/gpt2\"  # 使用较小的模型作为示例\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 显示具体的类型\n",
    "print(type(model))\n",
    "\n",
    "# 设置参数\n",
    "top_k = 50\n",
    "top_p = 0.7\n",
    "temperature = 0.7\n",
    "max_length = 200\n",
    "text = 'Once upon a time,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the people of the world were able to realize that they had been wronged by the gods. The gods had given them power over their own souls, and they were not alone in this.\n",
      "\n",
      "There are many reasons why we need to consider the possibility that our ancestors might have been wronged by the gods. One is that we do not know how much of our past history was based on these beliefs. Many of us believe that the gods did not give us any knowledge of how to survive. Others believe that we were born into the same universe as the gods, and that the gods gave us powers to control our bodies. Some of us believe that the gods created us to fight for the good of mankind, but others believe that they made us to live in the darkness of the night. We also believe that the gods gave us power to create new worlds, but they did not make us immortal.\n",
      "\n",
      "One of the most important things we can learn from the ancient Greeks is that the godsgenerate_with_sampling function:\n",
      "Time taken: 13.676683902740479 seconds\n",
      "Max Memory Usage: 1254416.0 MB\n",
      "User Mode CPU Time: 16.669843 seconds\n",
      "System Mode CPU Time: 0.957884 seconds\n"
     ]
    }
   ],
   "source": [
    "def _trim_past_key_values(past, max_length):\n",
    "    # 裁剪 past_key_values 以保留最近的 token 信息\n",
    "    return tuple(\n",
    "        tuple(p[:, :, :max_length, :].contiguous() for p in layer)\n",
    "        for layer in past\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_sampling(model, tokenizer, prompt, max_new_tokens, **kwargs):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 初始化 logits processors\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        TemperatureLogitsWarper(temperature),\n",
    "        TopKLogitsWarper(top_k=top_k),\n",
    "        TopPLogitsWarper(top_p=top_p),\n",
    "        RepetitionPenaltyLogitsProcessor(penalty=1.2)\n",
    "    ])\n",
    "\n",
    "    attention_mask = torch.ones(\n",
    "        input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "    cache_position = torch.arange(input_ids.shape[1], device=input_ids.device)\n",
    "\n",
    "    model_kwargs = {'use_cache': True,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'cache_position': cache_position,\n",
    "                    'past_key_values': None}\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 构建传入的生成参数\n",
    "        model_input = model.prepare_inputs_for_generation(input_ids=input_ids,\n",
    "                                                          **model_kwargs)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model.forward(**model_input,\n",
    "                                return_dict=True,\n",
    "                                output_attentions=False,\n",
    "                                output_hidden_states=False)\n",
    "\n",
    "        # 应用 logits processors\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_logits = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # 使用多项分布采样下一个 token\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "        # 更新生成的序列\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "\n",
    "        # 检查是否生成了结束标记\n",
    "        if next_tokens.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # 更新缓存等其他操作\n",
    "        model_kwargs = model._update_model_kwargs_for_generation(\n",
    "            outputs=outputs,\n",
    "            model_kwargs=model_kwargs,\n",
    "            is_encoder_decoder=False,\n",
    "        )\n",
    "\n",
    "        past = model_kwargs['past_key_values']\n",
    "\n",
    "        # print(f\"Input ids shape: {input_ids.shape}\")\n",
    "        # print(f\"Attention mask shape: {model_kwargs['attention_mask'].shape}\")\n",
    "        # print(f\"Past key values shape: {past[0][0].shape if past else 'None'}\")\n",
    "\n",
    "        yield tokenizer.decode(next_tokens[0])\n",
    "        if torch.backends.mps.is_available():\n",
    "            gc.collect()\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "# 监控generate_with_sampling函数的资源使用\n",
    "start_time = time.time()\n",
    "\n",
    "for generated_text in generate_with_sampling(\n",
    "    model, tokenizer, text, max_length):\n",
    "    print(f\"{generated_text}\",end='')\n",
    "end_time = time.time()\n",
    "print(\"generate_with_sampling function:\")\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the Lord said, \"I will send you to the land of the living, where you will live forever.\" And they lived for a long time. And they came to the land of the living. And they were not able to live for long. And the Lord said, \"Behold, I will send you to the land of the living, where you will live forever.\" And they lived for a long time. And they came to the land of the living. And they were not able to live for long. And the Lord said, \"Behold, I will send you to the land of the living, where you will live forever.\" And they lived for a long time. And they came to the land of the living. And they were not able to live for long. And the Lord said, \"Behold, I will send you to the land of the living, where you will live forever.\" And they lived for a long time. And they came to the land of thegenerate_with_sampling function:\n",
      "Time taken: 14.982378005981445 seconds\n",
      "Max Memory Usage: 1254416.0 MB\n",
      "User Mode CPU Time: 43.698964 seconds\n",
      "System Mode CPU Time: 1.681638 seconds\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_with_sampling(model, tokenizer, prompt, max_new_tokens, **kwargs):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 获取词嵌入层\n",
    "    embeddings = model.get_input_embeddings()\n",
    "    \n",
    "    # 初始化 logits processors\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        TemperatureLogitsWarper(temperature),\n",
    "        TopKLogitsWarper(top_k=top_k),\n",
    "        TopPLogitsWarper(top_p=top_p),\n",
    "        RepetitionPenaltyLogitsProcessor(penalty=1.2)\n",
    "    ])\n",
    "\n",
    "    attention_mask = torch.ones(\n",
    "        input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "    cache_position = torch.arange(input_ids.shape[1], device=input_ids.device)\n",
    "\n",
    "    model_kwargs = {'use_cache': True,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'cache_position': cache_position,\n",
    "                    'past_key_values': None}\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 构建传入的生成参数\n",
    "        model_input = model.prepare_inputs_for_generation(input_ids=input_ids,\n",
    "                                                          **model_kwargs)\n",
    "\n",
    "        # 将input_ids转换成input_embs并剔除\n",
    "        input_ids = model_input['input_ids']\n",
    "        del model_input['input_ids']\n",
    "        \n",
    "        model_input['inputs_embeds'] = embeddings(input_ids)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model.forward(**model_input,\n",
    "                                return_dict=True,\n",
    "                                output_attentions=False,\n",
    "                                output_hidden_states=False)\n",
    "\n",
    "        # 应用 logits processors\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_logits = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # 使用多项分布采样下一个 token\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "        # 更新生成的序列\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "\n",
    "        # 检查是否生成了结束标记\n",
    "        if next_tokens.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # 更新缓存等其他操作\n",
    "        model_kwargs = model._update_model_kwargs_for_generation(\n",
    "            outputs=outputs,\n",
    "            model_kwargs=model_kwargs,\n",
    "            is_encoder_decoder=False,\n",
    "        )\n",
    "\n",
    "        yield tokenizer.decode(next_tokens[0])\n",
    "        if torch.backends.mps.is_available():\n",
    "            gc.collect()\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "# 监控generate_with_sampling函数的资源使用\n",
    "start_time = time.time()\n",
    "\n",
    "for generated_text in generate_with_sampling(\n",
    "    model, tokenizer, text, max_length):\n",
    "    print(f\"{generated_text}\",end='')\n",
    "end_time = time.time()\n",
    "print(\"generate_with_sampling function:\")\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.get_input_embeddings()\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
