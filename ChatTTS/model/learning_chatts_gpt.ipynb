{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 17:09:46,307 - modelscope - INFO - PyTorch version 2.1.0 Found.\n",
      "2024-07-08 17:09:46,308 - modelscope - INFO - Loading ast index from /Users/charslee/.cache/modelscope/ast_indexer\n",
      "2024-07-08 17:09:46,336 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 8e4efa69aee288a831cd8dd27b421a93 and a total number of 972 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all load done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import resource\n",
    "from gpt import GPT_warpper\n",
    "import torch\n",
    "# 挑选最适合的设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('mirror013/ChatTTS')\n",
    "\n",
    "model_config = {\n",
    "    'num_audio_tokens': 626,\n",
    "    'num_text_tokens': 21178,\n",
    "    'gpt_config': {\n",
    "        'hidden_size': 768,\n",
    "        'intermediate_size': 3072,\n",
    "        'num_attention_heads': 12,\n",
    "        'num_hidden_layers': 20,\n",
    "        'use_cache': False,\n",
    "        'max_position_embeddings': 4096,\n",
    "        'spk_emb_dim': 192,\n",
    "        'spk_KL': False,\n",
    "        'num_audio_tokens': 626,\n",
    "        'num_text_tokens': None,\n",
    "        'num_vq': 4\n",
    "    }\n",
    "}\n",
    "# 加载模型\n",
    "model = GPT_warpper(**model_config).to(device).eval()\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir,\"asset/GPT.pt\"),map_location='cpu'))\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = torch.load(os.path.join(model_dir,\"asset/tokenizer.pt\"), map_location='cpu')\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# 设置参数\n",
    "\n",
    "# 设置参数\n",
    "top_k = 50\n",
    "top_p = 0.7\n",
    "temperature = 0.7\n",
    "max_length = 200\n",
    "text = \"你好呀旅行者\"\n",
    "\n",
    "print(\"all load done.\")\n",
    "def print_resource_usage():\n",
    "    # 获取最大内存使用量（以字节为单位）\n",
    "    max_memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    # 打印内存使用情况\n",
    "    print(f\"Max Memory Usage: {max_memory_usage / 1024} MB\")\n",
    "\n",
    "    # 获取用户模式和系统模式下的CPU时间\n",
    "    user_time, sys_time = resource.getrusage(resource.RUSAGE_SELF)[:2]\n",
    "    # 打印CPU时间\n",
    "    print(f\"User Mode CPU Time: {user_time} seconds\")\n",
    "    print(f\"System Mode CPU Time: {sys_time} seconds\")\n",
    "\n",
    "        # 如果CUDA设备可用，打印CUDA内存使用情况\n",
    "    if torch.cuda.is_available():\n",
    "        # 获取CUDA设备的总内存和空闲内存\n",
    "        free_memory, total_memory = torch.cuda.mem_get_info()\n",
    "        # 计算已使用的内存\n",
    "        used_memory = total_memory - free_memory\n",
    "        # 打印CUDA内存使用情况\n",
    "        print(f\"CUDA Memory Usage: {used_memory / 1024**2} MB used out of {total_memory / 1024**2} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备input_ids等前置工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:267\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# 监控generate_with_sampling函数的资源使用\u001b[39;00m\n\u001b[1;32m     76\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generated_text \u001b[38;5;129;01min\u001b[39;00m generate_with_sampling(\n\u001b[1;32m     79\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mtext, max_new_tokens\u001b[38;5;241m=\u001b[39mmax_length):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mgenerate_with_sampling\u001b[0;34m(prompt, max_new_tokens, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 初始化 logits processors\u001b[39;00m\n\u001b[1;32m     21\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m LogitsProcessorList([\n\u001b[1;32m     22\u001b[0m     TemperatureLogitsWarper(temperature),\n\u001b[1;32m     23\u001b[0m     TopKLogitsWarper(top_k\u001b[38;5;241m=\u001b[39mtop_k),\n\u001b[1;32m     24\u001b[0m     TopPLogitsWarper(top_p\u001b[38;5;241m=\u001b[39mtop_p),\n\u001b[1;32m     25\u001b[0m     RepetitionPenaltyLogitsProcessor(penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n\u001b[1;32m     26\u001b[0m ])\n\u001b[1;32m     28\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[0;32m---> 29\u001b[0m     \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     30\u001b[0m cache_position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask,\n\u001b[1;32m     34\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m'\u001b[39m: cache_position,\n\u001b[1;32m     35\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:269\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.generation.logits_process import LogitsProcessorList, TemperatureLogitsWarper, TopKLogitsWarper, TopPLogitsWarper, RepetitionPenaltyLogitsProcessor\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_sampling(prompt, max_new_tokens, **kwargs):\n",
    "\n",
    "    if not isinstance(prompt, list):\n",
    "        prompt = [prompt]\n",
    "\n",
    "    # 添加必要的标签\n",
    "    prompt = [f'[Stts][empty_spk]{i}[Ptts]' for i in prompt]\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "\n",
    "    # 初始化 logits processors\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        TemperatureLogitsWarper(temperature),\n",
    "        TopKLogitsWarper(top_k=top_k),\n",
    "        TopPLogitsWarper(top_p=top_p),\n",
    "        RepetitionPenaltyLogitsProcessor(penalty=1.2)\n",
    "    ])\n",
    "\n",
    "    attention_mask = torch.ones(\n",
    "        input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "    cache_position = torch.arange(input_ids.shape[1], device=input_ids.device)\n",
    "\n",
    "    model_kwargs = {'use_cache': True,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'cache_position': cache_position,\n",
    "                    'past_key_values': None}\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 构建传入的生成参数\n",
    "        model_input = model.prepare_inputs_for_generation(input_ids=input_ids,\n",
    "                                                          **model_kwargs)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model.forward(**model_input,\n",
    "                                return_dict=True,\n",
    "                                output_attentions=False,\n",
    "                                output_hidden_states=False)\n",
    "\n",
    "        # 应用 logits processors\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_logits = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # 使用多项分布采样下一个 token\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "        # 更新生成的序列\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "\n",
    "        # 检查是否生成了结束标记\n",
    "        if next_tokens.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # 更新缓存等其他操作\n",
    "        model_kwargs = model._update_model_kwargs_for_generation(\n",
    "            outputs=outputs,\n",
    "            model_kwargs=model_kwargs,\n",
    "            is_encoder_decoder=False,\n",
    "        )\n",
    "\n",
    "        yield tokenizer.decode(next_tokens[0])\n",
    "        if torch.backends.mps.is_available():\n",
    "            gc.collect()\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "\n",
    "# 监控generate_with_sampling函数的资源使用\n",
    "start_time = time.time()\n",
    "\n",
    "for generated_text in generate_with_sampling(\n",
    "        prompt=text, max_new_tokens=max_length):\n",
    "    print(f\"{generated_text}\", end='')\n",
    "end_time = time.time()\n",
    "print(\"generate_with_sampling function:\")\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print_resource_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
