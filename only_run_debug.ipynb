{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from IPython.display import Audio"]},{"cell_type":"markdown","metadata":{},"source":["型下载"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from modelscope import snapshot_download\n","model_dir = snapshot_download('mirror013/ChatTTS')"]},{"cell_type":"markdown","metadata":{},"source":["加载模型"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import ChatTTS\n","chat = ChatTTS.Chat()\n","chat.load_models(\n","    source=\"local\",\n","    local_path=model_dir,\n","    device='cpu',\n","    compile=False,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["推理并返回梅尔声谱图"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mel_spec = chat.infer(\n","    text='你好，我是Chat T T S。',\n","    use_decoder=False,\n","    return_mel_spec=True,\n",")[0]\n","print(mel_spec.shape)"]},{"cell_type":"markdown","metadata":{},"source":["绘制梅尔声谱图"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(10, 4))\n","plt.imshow(mel_spec[0].detach().numpy(), aspect='auto', origin='lower')\n","plt.title('Log-Mel Spectrogram')\n","plt.xlabel('Time')\n","plt.ylabel('Mel Frequency')\n","plt.colorbar(format='%+2.0f dB')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import librosa\n","# 这里我们取第一个 batch 的 Mel 频谱图\n","mel_spectrogram = mel_spec[0].detach().numpy()\n","\n","# 转换 Mel 频谱图为功率谱\n","mel_spectrogram = np.exp(mel_spectrogram) - 1\n","\n","# 使用 librosa 的 mel_to_audio 函数将 Mel 频谱图转换为音频信号\n","audio = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=24000)\n","\n","# 播放音频\n","Audio(audio, rate=24000,autoplay=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 获取中间结果\n","infer_middle_result = chat.infer(\n","    text='你好，我是ChatTTS。', \n","    skip_refine_text=True, \n","    refine_text_only=True, \n","    return_infer_token=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for key,value in infer_middle_result.items():\n","    print(f\"+----------{key}------------+\")\n","    for v in value:\n","        if v is None: continue\n","        print(f\"{v.shape}\")\n","    print(f\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(infer_middle_result['hiddens'][0].shape)\n","print(infer_middle_result['hiddens'][0][None].permute(0,2,1).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 保存chatts.npy\n","import numpy as np\n","print(infer_middle_result['hiddens'][0][None].permute(0,2,1).shape)\n","print(infer_middle_result['hiddens'][0][None].permute(0,2,1).dtype)\n","np.save(\"chatts.npy\", infer_middle_result['hiddens'][0][None].permute(0,2,1).cpu().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["# fish-speech 音色迁移\n","2024-06-02 20:23:02.733 | INFO     | __main__:load_model:39 - Restored model from checkpoint\n","2024-06-02 20:23:02.740 | INFO     | __main__:main:96 - Processing precomputed indices from codes_0.npy\n","2024-06-02 20:23:02.747 | INFO     | __main__:main:100 - indices shape: torch.Size([2, 81])\n","2024-06-02 20:23:03.880 | INFO     | __main__:main:114 - Loaded reference audio from zhongli.ogg, shape: torch.Size([435207])\n","2024-06-02 20:23:13.913 | INFO     | __main__:main:126 - Encoded text: torch.Size([1, 25])\n","2024-06-02 20:23:13.915 | INFO     | __main__:main:130 - indices shape: torch.Size([2, 81])      feature_lengths:tensor([81], device='cuda:0')\n","2024-06-02 20:23:14.220 | INFO     | __main__:main:134 - Restored VQ features: torch.Size([1, 768, 324])\n","2024-06-02 20:23:14.773 | INFO     | __main__:main:144 - Generated audio: torch.Size([1, 1, 165888]), equivalent to 3.76 seconds\n","2024-06-02 20:23:14.773 | INFO     | __main__:main:150 - fake_audio shape: (165888,)\n","2024-06-02 20:23:14.778 | INFO     | __main__:main:152 - Saved audio to generate0.wav"]},{"cell_type":"markdown","metadata":{},"source":["# dvae 模型架构\n","```\n","DVAE(\n","  (decoder): DVAEDecoder(\n","    (conv_in): Sequential(\n","      (0): Conv1d(512, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (1): GELU(approximate='none')\n","      (2): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n","    )\n","    (decoder_block): ModuleList(\n","      (0-11): 12 x ConvNeXtBlock(\n","        (dwconv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(6,), dilation=(2,), groups=256)\n","        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n","        (act): GELU(approximate='none')\n","        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n","      )\n","    )\n","    (conv_out): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n","  )\n","  (out_conv): Conv1d(512, 100, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n","  (vq_layer): GFSQ(\n","    (quantizer): GroupedResidualFSQ(\n","      (rvqs): ModuleList(\n","        (0-1): 2 x ResidualFSQ(\n","          (project_in): Linear(in_features=512, out_features=4, bias=True)\n","          (project_out): Linear(in_features=4, out_features=512, bias=True)\n","          (layers): ModuleList(\n","            (0-1): 2 x FSQ(\n","              (project_in): Identity()\n","              (project_out): Identity()\n","            )\n","          )\n","        )\n","      )\n","    )\n","  )\n",")\n","```"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
